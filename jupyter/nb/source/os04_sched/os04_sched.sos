<!--- md --->

#* オペレーティングシステム 演習 04

#* スケジューリング

<!--- end md --->

<!--- md w --->

名前と学生証番号を書け. Enter your name and student ID.

 * 名前 Name:
 * 学生証番号 Student ID:

<!--- end md --->

<!--- md --->
# スケジューリングの記録と可視化
<!--- end md --->

<!--- md --->

* スレッド(プロセス)が脇目もふらず実行をしているつもりでも, そのようなスレッドが(仮想)コア数以上あれば, それらすべてのスレッドが, 同時に実行されているはずはない. 
* OSは, 時折スレッドをCPUコア上で切り替えながら実行しているはずである. その様子を観測する実験をしてみよう.
<!--- end md --->

<!--- md --->

以下のプログラムは擬似コードで書けば以下のようなことをしている

```
 1: int main() {
 2:   t = 現在時刻();
 3:   while (10秒たつか記録する配列があふれるまで) {
 4:     t' = 現在時刻();
 5:     t'と, 最後に見た現在時刻(t)を比べる;
 6:     tとt'の差が非常に小さければ, 自分はtからt'までCPUが割り当てられていた,
 7:     そうでなければ, tからt'まで自分は走っていなかった
 8:     (CPUが他のスレッドに割り当てられていた)と考える;
 9:     t = t';
10:   }
11:   CPUが自分に割り当てられていたと考えられる時間帯を表示する;
12: }
```

* スレッドは実質的に時計を見る以外のことをせずにひたすら走り続けている
* したがってほんとんどの時, tとt'の差は小さいのだが稀に, そうでないことがある
* それはおそらくその間他のスレッドにCPUが割り当てられており自分はその時間帯OSによって止められていた場合に生ずる

<!--- end md --->

<!--- md --->

* 以下が実際のCコード

<!--- end md --->

<!--- code w kernel=python --->
%%writefile sched_rec.c
<!--- exec-include ./mk_version.py -DVER=1 nb/source/os04_sched/include/sched_rec.c --->
<!--- end code --->

<!--- code w kernel=bash --->
gcc -Wall -o sched_rec sched_rec.c
<!--- end code --->

<!--- md --->

* 以下は1秒ほど, 単独で走らせてみる例
* 1行の読み方は以下
```
プロセスID  区間開始時刻  区間終了時刻  CPU番号 区間の長さ
```
* なお, Jupyterのbash kernelのバグによって出力が出ない場合があるかもしれない(経験上, 最初の一回がそうなることが多い). そういうときは何度か実行してみてください

<!--- end md --->

<!--- code w kernel=bash --->
./sched_rec 1
<!--- end code --->

<!--- md ---> 

* 各行はひとつの, 連続してOSにCPUを与えられていた時間(区間)を表している
* もし上記の結果が1行しか表示されなければそれは, 約1秒間, まったく(正確には1ms以上連続して)他のスレッドにCPUを奪われずに走っていたということになる(最後の行がほぼ1秒であることを確認せよ)
* 2行以上に渡った場合, ある区間の終了時刻と次の区間の開始時刻の差を観察してみよ(最後の行を合計するとほぼ1秒になることを確認せよ)

<!--- end md --->

<!--- md --->

* 以下ではこのプロセスを複数同時に走らせると何が起きるかを観察, 可視化する
* その前に以下で, このJupyter環境(をホストしてるマシン)にいくつのCPU (正確には仮想コア)がつまれているかを見てみる

<!--- end md --->

<!--- code w kernel=bash --->
lscpu
<!--- end code --->

<!--- md --->

* 以下はシェルスクリプトで同時に4個, 3秒間このプロセスを走らせ, 結果はファイル sched.0, sched.1, sched.2, sched.3 に記録したもの

<!--- end md --->

<!--- code w kernel=bash --->
./sched_rec 3 > sched.0 | ./sched_rec 3 > sched.1 | ./sched_rec 3 > sched.2 | ./sched_rec 3 > sched.3
<!--- end code --->

<!--- md --->

# スケジューリングの可視化

* sched_vis は sched_rec の結果を可視化する関数
* 同時に結果は sched.svg という画像ファイルに保存される

<!--- end md --->

<!--- code w kernel=python --->
<!--- include nb/source/os04_sched/include/sched_vis.py --->
<!--- end code --->

<!--- md --->
* 以下で実際に可視化
* 以下は sched.0, sched.1, sched.2, sched.3 の4つのファイルを可視化する
* start_t, end_tを指定すれば可視化の開始時刻と終了時刻を指定できる(start_t=None, end_t=Noneにすると, それぞれファイル中に含まれる最小, 最大の時刻という意味)
<!--- end md --->

<!--- code w kernel=python --->
sched_vis(["sched.{}".format(i) for i in range(4)])
#sched_vis(["sched.{}".format(i) for i in range(4)], start_t=1.0, end_t=1.5)
<!--- end code --->

<!--- md --->

* 本題は, プロセスをさらに増やしたらどうなるかということ
* 上記の書き方で複数同時に走らせるのはタイプ量の限界を超えるので, シェルスクリプトのループを使う
* 以下は8個のプロセスを3秒間走らせる
* 記録はsched.0 ... sched.7 に記録される
<!--- end md --->

<!--- code w kernel=bash --->
for i in $(seq 0 7); do
  taskset -c 0-3 ./sched_rec 3 > sched.${i} &
done
wait
<!--- end code --->

<!--- md --->

* <font color="blue">実行し終えたら再び, 可視化せよ</font>

<!--- end md --->

<!--- code w kernel=python --->
sched_vis(["sched.{}".format(i) for i in range(8)])
#sched_vis(["sched.{}".format(i) for i in range(8)], start_t=1.0, end_t=1.5)
<!--- end code --->

<!--- md --->
* sched.svg というファイルが生成されているのでそれを開いて, 適宜拡大表示してみよ
<!--- end md --->


<!--- md --->

* 上記の taskset コマンドはプロセスを特定のCPUでしか実行できないようにするコマンド(man参照)
```
taskset -c 0-3 ./sched_rec 3
```
はCPU 0 ... 3 を使えということ. <font color="blue">tasksetの引数を変えて結果の違いを観察せよ</font>
* プロセス数を 使うCPU数より少し少ないあたりから, 使うCPU数x2くらいまで, <font color="blue">色々変えて結果の違いを観察せよ</font>

<!--- end md --->

<!--- md --->

* 注: 大勢で同じ環境を共有しているので, 複数の人が同時に同じ実験をすると自分は少数のプロセスしか立ち上げていなくても, 他の人のプロセスにCPUを奪われているかも知れない
* それ以外にもこのJupyterノートブック自身の処理やWebサーバの処理も行われるのでそれらにCPUを奪われることもある
* 従って, 同じ設定で実験しても結果は毎回同じとは限らない
* 自分だけの環境で走らせると結果の予測はしやすい

<!--- end md --->

<!--- md --->
## PythonでもCでも同じ

* プログラムがどの言語で書かれていてもOSがやることは同じであることを見せておく
* sched_rec.c と同じことをする Python プログラム

<!--- end md --->

<!--- code w kernel=python --->
%%writefile sched_rec.py
<!--- exec-include ./mk_version.py -DVER=1 nb/source/os04_sched/include/sched_rec.py --->
<!--- end code --->

<!--- code md --->
* 上記と同様, 8 プロセスを4 仮想コア上で走らせる例
<!--- end code --->

<!--- code w kernel=bash --->
for i in $(seq 0 7); do
  taskset -c 0-3 python3 sched_rec.py 3 > sched.${i} &
done
wait
<!--- end code --->

<!--- md --->

* 可視化

<!--- end md --->

<!--- code w kernel=python --->
sched_vis(["sched.{}".format(i) for i in range(8)])
#sched_vis(["sched.{}".format(i) for i in range(8)], start_t=1.0, end_t=1.5)
<!--- end code --->

<!--- md --->
# Linux CFSスケジューラ
<!--- end md --->

<!--- md --->

* OSによるスケジューリングの基本
  * 中断しているスレッドと実行可能なスレッドを区別する
  * 実行可能なスレッドに公平にCPU時間を割り当てる

* 公平にCPU時間を割り当てる方法は色々あるがLinux のデフォルトスケジューラである, Complete Fair Scheduler (CFS) はCPUの利用時間累計を記録して, スレッド切り替え(実行中スレッドが中断した, 中断中スレッドが復帰した, タイマ割り込みがおきたなど)のタイミングで, それが最も少ないものに割り当てるという自然なもの
* CFSではCPUの利用時間累計を, スレッドごとに vruntime という変数を割り当てて管理してる. すなわち使ったCPU時間だけvruntimeを加算し, 次に実行するスレッドを選ぶときにvruntime最小のスレッドにCPU時間を割り当てるのがCFSの基本である
* しかしvruntimeに文字通りのCPU利用時間累計を記録していくと, 生まれたばかりのスレッドはvruntime=0, また, ほとんどCPUを使わずにずっと中断していたスレッドも vruntime $\approx$ 0ということになり, それらのスレッドが急に計算を始めると, それらに長時間CPUが連続して割り当てられる(すでにCPUを10秒消費しているスレッドには10秒間順番がまわってこない)ということになりかねない
* したがってvruntimeは以下のような管理がなされる

1. 親スレッドAが子スレッドBを生成した時, BはAのvruntimeを引き継ぐ (プロセスの場合も同様)
   Bのvruntime = Aのvruntime
2. 実行中のスレッドAから別のスレッドBに実行が切り替わる(コンテクストスイッチ)時
   Aのvruntime += Aが今回消費した時間
3. 中断中のスレッドAが復帰する時
   Aのvruntime += max(Aのvruntime, min { tのvruntime | t : 実行可能なスレッド})

<!--- end md --->

<!--- md --->

# Vruntime の記録と可視化

* 以下のプログラムはvruntimeをずっと観測し続け, 時間とともにどう変化したかを記録する
* 以下が実際のCコード

<!--- end md --->

<!--- code w kernel=python --->
%%writefile vruntime_rec.c
<!--- exec-include ./mk_version.py -DVER=1 nb/source/os04_sched/include/vruntime_rec.c --->
<!--- end code --->

<!--- code w kernel=bash --->
gcc -Wall -o vruntime_rec vruntime_rec.c
<!--- end code --->

<!--- md --->

* 以下は1秒ほど, 単独で走らせてみて, 出力の最初の10行を表示する例
* 出力1行の読み方は以下
```
プロセスID CPU番号 区間開始時刻  区間終了時刻  区間の長さ その間のvruntime
```
* 各行は, vruntimeが変わらなかった区間を表している

<!--- end md --->

<!--- code w kernel=bash --->
taskset -c 2 ./vruntime_rec 3 > vr.0
head vr.0
<!--- end code --->

<!--- md ---> 

* なお以下の taskset -c 2 は processor (仮想コア) 2番 だけで実行せよという指示
* これはそのスレッドが1つの仮想コアでしか実行されないようにするための仕掛け(2番であることには意味はない)
* vruntime はprocessorごとに別々に管理されているため, スレッドがprocessor間を移動するとvruntimeは急に変化する(それはそれで見ておくと良い)

<!--- end md ---> 

<!--- md ---> 

# Vruntimeの可視化

* 以下が結果を可視化するプログラム

<!--- end md ---> 

<!--- code w kernel=python --->
<!--- include nb/source/os04_sched/include/vruntime_vis.py --->
<!--- end code ---> 

<!--- md --->

* 以下で実際に可視化

<!--- end md --->

<!--- code w kernel=python points=1 --->
vruntime_vis(["vr.0"])
<!--- end code --->

<!--- md --->
* 特定の領域を可視化するには, `start_t=開始時刻, end_t=終了時刻` を与える
* 一見して連続的に変化しているように見えるが, 狭い範囲をどんどん拡大してみると, 区間ごとに一定で, それが定期的に, 階段状に増加していることがわかる
* 以下の引数を変更して小さな区間を拡大表示してみよ

* 下記を実行するとvruntime.svgというSVG (ベクタ画像)ファイルが生成されるので, ある程度まで拡大表示した画像をダウンロードして適当な画像ビューアで拡大しても良い

<!--- end md --->

<!--- code w kernel=python --->
vruntime_vis(["vr.0"], start_t=1.0, end_t=1.5)
<!--- end code --->

<!--- md --->
# taskset でスレッドをprocessorに固定しない場合

* taskset でスレッドを特定のprocessorに固定しない場合, スレッドがprocessor間を動き回れるようになる
* 実際にスレッドがprocessor間を移動するかはわからないが, 途中でvruntime値が大きくジャンプしたらそれはおそらくprocessor (仮想コア)間をスレッドが移動したとき

<!--- end md --->

<!--- code w kernel=bash points=1 --->
./vruntime_rec 3 > vr.0
<!--- end code --->

<!--- code w kernel=python points=1 --->
vruntime_vis(["vr.0"])
# vruntime_vis(["vr.0"], start_t=1.0, end_t=1.5)
<!--- end code ---> 

<!--- md --->

# 複数のスレッドがいる場合

* 以下はシェルスクリプトで同時に4個, 3秒間このプロセスを走らせ, 結果はファイル vr.0, vr.1, vr.2, vr.3 に記録したもの
* tasksetコマンドですべて同じprocessor (仮想コア) で実行するようにしている

<!--- end md --->

<!--- code w kernel=bash points=1 --->
for i in $(seq 0 3); do
  taskset -c 2 ./vruntime_rec 3 > vr.${i} &
done
wait
<!--- end code --->

<!--- md --->

* 実行し終えたら再び可視化せよ
* 適切に短い区間を拡大表示するなどして4つのプロセスのvruntimeがどのように増えていくかを観察せよ

<!--- end md --->

<!--- code w kernel=python ---> 
vruntime_vis(["vr.0", "vr.1", "vr.2", "vr.3"])
#vruntime_vis(["vr.0", "vr.1", "vr.2", "vr.3"], start_t=1.0, end_t=1.5)
<!--- end code ---> 

<!--- md --->
* なお, プロセス数が多いときにも通用するもう少しスマートな書き方は以下 (これは純粋にPythonの話)
<!--- end md --->

<!--- code w kernel=python ---> 
vruntime_vis([f"vr.{i}" for i in range(4)])
#vruntime_vis([f"vr.{i}" for i in range(4)], start_t=1.0, end_t=1.5)
<!--- end code ---> 

<!--- md --->

# Blockしている間はvruntimeは増えない

* 「vruntimeはCPUを使っていない時間は増えない」ことを確かめるために, 先のプログラムを少し走っては少し休むように変更したものが以下

```
./vruntime_rec_slp T R S
```

とすると, 「R秒走ってS秒sleepする」を, 合計T秒間繰り返す

<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile vruntime_rec_slp.c
<!--- exec-include ./mk_version.py -DVER=2 nb/source/os04_sched/include/vruntime_rec.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
gcc -Wall -o vruntime_rec_slp vruntime_rec_slp.c
<!--- end code --->

<!--- md --->
* 実行
<!--- end md --->

<!--- code w kernel=bash points=1 --->
taskset -c 2 ./vruntime_rec_slp 3 0.2 0.1 > vr.0
<!--- end code --->

<!--- md --->
* vruntime可視化
<!--- end md --->

<!--- code w kernel=python --->
vruntime_vis(["vr.0"])
#vruntime_vis(["vr.0"], start_t=1.0, end_t=1.5)
<!--- end code ---> 

<!--- md --->

* ずっとCPUを使っているスレッドの傍ら, 少し走っては少しsleepを繰り返すスレッドがいた場合, 後者がsleepから目覚めたときのvruntimeには何が起きているか
* 以下は
  * 0.2秒走って0.1秒sleep する (./vruntime_rec_slp 3 0.2 0.1) を1つと
  * ずっと走る (./vruntime_rec_slp 3 3 0) を3つ
同時に走らせる

<!--- end md --->

<!--- code w kernel=bash --->
for i in $(seq 0 3); do
  if [ ${i} = 0 ]; then
    taskset -c 2 ./vruntime_rec_slp 3 0.2 0.1 ;
  else
    taskset -c 2 ./vruntime_rec_slp 3 3 0 ;
  fi > vr.${i} &
done
wait
<!--- end code --->

<!--- md --->
* 可視化
<!--- end md --->

<!--- code w kernel=python ---> 
vruntime_vis([f"vr.{i}" for i in range(4)])
#vruntime_vis([f"vr.{i}" for i in range(4)], start_t=1.0, end_t=1.5)
<!--- end code ---> 


<!--- md --->
# niceの効果

* niceというコマンドならびにシステムコールがある
* プロセスの「nice値」を決めるもので, nice値が高い = 他のプロセスに実行をよく譲る(だからnice) = 優先度を非掬するというもの

```
nice -19 コマンド
```

は「コマンド」の「nice値」を19 (最大)にする. 同名のシステムコールもある.

* デフォルトのnice値は0. 正の値は多少なりとも優先度を下げるという意味になる

* nice値大きいプロセスと普通のプロセスが並走すると, ほとんどの時間を後者が消費するようになる

* CFSスケジューラでniceがどのように実現されているのかをvruntimeを観測することで考えてみよ
<!--- end md --->

<!--- code w kernel=bash --->
for i in $(seq 0 3); do
  if [ ${i} = 0 ]; then
    taskset -c 2 nice -5 ./vruntime_rec 3 ;
  else
    taskset -c 2 ./vruntime_rec 3 ;
  fi > vr.${i} &
done
wait
<!--- end code --->

<!--- code w kernel=python ---> 
vruntime_vis([f"vr.{i}" for i in range(4)])
#vruntime_vis([f"vr.{i}" for i in range(4)], start_t=1.0, end_t=1.5)
<!--- end code --->


