<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ja" lang="ja">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
<meta http-equiv="content-script-type" content="text/javascript" />
<meta http-equiv="content-style-type" content="text/css" />
<meta name="author" content="haraken" />
<meta name="description" content="オペレーティングシステム スレッドプログラミング演習" />
<meta name="keywords" content="" />
<title>
オペレーティングシステム 並行プログラミング演習
</title>


<style>
body {counter-reset: h2}
  h2 {counter-reset: h3}
  h3 {counter-reset: h4}
  h4 {counter-reset: h5}
  h5 {counter-reset: h6}

  h2:before {counter-increment: h2; content: counter(h2) ". "}
  h3:before {counter-increment: h3; content: counter(h2) "." counter(h3) ". "}
  h4:before {counter-increment: h4; content: counter(h2) "." counter(h3) "." counter(h4) ". "}
  h5:before {counter-increment: h5; content: counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) ". "}
  h6:before {counter-increment: h6; content: counter(h2) "." counter(h3) "." counter(h4) "." counter(h5) "." counter(h6) ". "}

  h2.nocount:before, h3.nocount:before, h4.nocount:before, h5.nocount:before, h6.nocount:before { content: ""; counter-increment: none } 

h1 {
  font-size   : 24pt;
  xfont-family : serif;
  margin      : 10pt;
  padding     : 3pt 20pt;
  border-style     : solid;
  border-width     : 1pt 1pt 0pt 15pt ;
   
  border-color     : #99A1AA;
  background-color : #DDDDEE;
}

h2 {
  font-size   : 20pt;
  xfont-family : serif;
  margin      : 10pt;
  padding     : 3pt 20pt;
  border-style     : solid;
  border-width     : 1pt 1pt 0pt 15pt ;
   
  border-color     : #99A1AA;
  background-color : #EEEEFF;
}

h3 {
  font-size   : 18pt;
  xfont-family : serif;
  margin      : 10pt;
  padding     : 3pt 20pt;
  border-style     : solid;
  border-width     : 1pt 1pt 0pt 15pt;
  border-color     : #99A1AA;
  background-color : #EEEEFF;
}

div {
  font-size   : 16pt;
  xfont-family : serif;
  margin      : 10pt;
  padding     : 3pt 20pt;
  border-color     : #99A1AA;
}

p {
  font-size   : 16pt;
  xfont-family : serif;
  border-color     : #99A1AA;
}

pre {
  background-color:#efe;
}

</style>

</head>
<body>

<!-- コンテンツはここから -->

<h1>並行プログラミング演習</h1>

<h2>はじめに</h2>
<div>
<p>
Unix (Linux)上でスレッドプログラミングを行う標準的なAPIである,
POSIX thread (Pthread)プログラミングの演習を行います.
スレッドは, 一つのプロセス(= プログラムを立ち上げた結果できるもの)
内に複数作ることが出来, 
それはオペレーティングシステムによってスケジュールされます.
一つのプロセス内のスレッドはメモリを共有します.
つまり, 
あるスレッドがアドレスAに書き込み, 別のスレッドが同じアドレスAから読み出せば,
両者の間で値を受け渡すことができます. 
ただし, スレッドがどういうタイミングでスケジュールされるかは分からないため,
値を正しく受け渡すには同期が必要です.
</p>
</div>

<!--
<h2>名前</h2>
<div>
以下に名前などを記入. 教室内で授業時間中に記入すること. 
<p>
<iframe src="https://docs.google.com/forms/d/1Z0rz-6vNfgp1aMmccJRWmf9KO-h97Y50fP-oaNBunl8/viewform?embedded=true" width="760" height="500" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
</p>
</div>


<h2>ウォームアップ</h2>
<div>
<p>
Pthreadの使い方を学ぶため, 
以下のプログラムをダウンロードし, コンパイル, 実行せよ.
<ul>
  <li><a href=pthread_empty.c>pthread_empty.c</a></li>
</ul>

一応やり方:
<blockquote>
<pre>
gcc pthread_empty.c -lpthread
</pre>
</blockquote>
</p>
</div>

<h2>変数のアドレスを確認する演習</h2>
<div>
<p>
前回の授業で見せた, 変数のアドレスを表示する例題を,
スレッドを作った上でやってみる. 
</p>

<p>
<font color=red>演習0:</font> 具体的には, 
pthread_empty.c をコピーし, 以下のように修正せよ.
<ul>
<li>大域変数(グローバル変数)gを定義する.
<li>thread_funcの中に局所変数(ローカル変数)xを作る.
<li>thread_funcの中に静的変数(static変数)sを作る.
<li>mainの中で局所変数yを作る
<li>thread_funcの中でそれらのアドレスを表示する
  (yのアドレスは, thread_arg構造体に入れて渡す).
</ul>
どのアドレスはスレッド毎に異なっており,
どのアドレスは全スレッドで共通かを確認せよ.
</li>

<p>
一つのプロセス中のスレッドは, 
アドレス空間を共有しており
(この言葉の意味はまだきちんと説明していない),
要は, アドレスが同じなら物理的に同じメモリの場所を使っている,
したがってメモリが共有されているということである.
</p>

<p>
<iframe src="https://docs.google.com/forms/d/18eQi3eVu0xqP_4HJ2_hQZIMfWC84Fmrr5NUiudNGudA/viewform?embedded=true" width="760" height="800" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
</p>
</div>

-->


<h2>ウォームアップ</h2>
<div>
<p>
Pthreadの使い方を学ぶため, 
以下のプログラムをダウンロードし, 
何をするプログラムか確認し，コンパイルし, 実行せよ.
<ul>
  <li><a href=pthread_hello.c>pthread_hello.c</a></li>
</ul>

一応やり方:
<blockquote>
<pre>
gcc pthread_hello.c -lpthread
</pre>
</blockquote>
</p>
</div>


<h2>おまけ: OpenMP</h2>
<div>
<p>
PthreadのAPIはOSが直接提供するインタフェースとして覚えておく価値があるが, 
使い始めるとすぐに面倒くさくなる. スレッドを作る, それにデータを渡す,
などの決まりきったことのために毎度似た様なことをしなくてはいけなくなる.
OpenMPという並列言語(といってもC言語を元にしたわずかな拡張)
を使うとこのような定型的な処理はもっと簡単に書ける. 端的に言うと,
<blockquote>
<pre>
#pragma omp parallel
  文
</pre>
</blockquote>
と書くだけで, 文を実行するスレッドがいくつか作られ,
それが終了するのを待つ(作られたスレッドが実行するのは
#pragma omp parallel「直後の」文．複数の文を実行したければ，
{ ... } でくくれば良い).
</p>

<p>
ここでいくつのスレッドを作るかはプログラム
起動時の環境変数OMP_NUM_THREADSで指定できる.
コンパイル時に -fopenmp というオプションをつけるとOpenMPが有効になる
(つまり上の #pragma omp parallel という指示が無視されずに解釈される).
つまり以下のようにしてコンパイルして実行する.
</p>

<blockquote>
<pre>
$ gcc -fopenmp openmp_hello.c
$ OMP_NUM_THREADS=5 ./a.out
</pre>
</blockquote>

<p>
OpenMPを使うと, pthread_createのインタフェースに
あわせるため(だけの)構造体の定義, pthread_createを呼び出す
ためのループ, スレッドの終了を待つためのループなど煩雑な作業を省略できる.
</p>

<blockquote>
<pre>
#pragma omp parallel
  f(a, b, c);
</pre>
</blockquote>
と書くと, だいたい以下と似たようなことが行われるとイメージすればよい.

コンパイラが以下のような構造体や関数を勝手に生成してくれる.
<blockquote>
<pre>
typedef struct thread_arg {
  int a;
  int b;
  int c;
} * thread_arg_t;

void * thread_func(void * _arg) {
  thread_arg_t arg = _arg;
  f(arg->a, arg->b, arg->c);
  return 0;
}
</pre>
</blockquote>

上記 #pragma omp parallel の実行自身は以下のように
pthread_createに置き換えられるとイメージしておけば良い．

<blockquote>
<pre>
  ...
  n_threads = atoi(getenv("OMP_NUM_THREADS"));
  for (int i = 0; i < n_threads; i++) {
    args[i].a = a;
    args[i].b = b;
    args[i].c = c;
    pthread_create(&args[i].tid, NULL, thread_func, (void *)&args[i]);
  }
  for (int i = 0; i < n_threads; i++) {
    pthread_join(args[i].tid);
  }
  ...
</pre>
</blockquote>

<p>
このようにOpenMPを使うと, pthread_createのインタフェースに
あわせるため(だけの)構造体の定義, pthread_createを呼び出す
ためのループ, スレッドの終了を待つためのループなど煩雑な作業を省略できる.
</p>


<p>
pthreadの変わりにOpenMPを使ってみたいという人は,
<a href=openmp_hello.c>openmp_hello.c</a>をダウンロードしてみよ.
pthread_hello.c と見比べれば単純であることがわかるだろう.
</p>

<p>
<font color=red>注 (去年の情報):</font> 
Macを使っている人は，コンパイル時に，
<pre>
fatal error: 'omp.h' file not found
</pre>
というエラーを見ることになる人も多いだろう．
これは，Macのgccがclang (LLVM)に置き換えられていて，OpenMPをサポートしてないからである．
今後の課題でも問題が生じるので，諦めてVMで実行したり，
電気系の学生であれば学科から支給されているPCを使ったりするほうが良い．
Macを使いたいという人は，HomebrewやMacPortsを使うなどして，
gccを新しくインストールすることで解決できる．
<pre>
$ brew install gcc49
$ gcc-4.9 -fopenmp openmp_hello.c
</pre>
</p>
</div>

<h2>競合状態を確認する演習</h2>
<div>

このプログラム<a href=pthread_vars.c>pthread_inc.c</a>
をダウンロードして，エディタで眺めよ．

<pre>
   ...

long g = 0;
void * thread_func(void * _arg) {
  thread_arg_t arg = _arg;
  int i;
  static long s = 0;
  long l = 0;
  for (i = 0; i < 10000000; i++) { g++; }
  for (i = 0; i < 10000000; i++) { s++; }
  for (i = 0; i < 10000000; i++) { l++; }
  for (i = 0; i < 10000000; i++) { arg->c[0]++; }
  for (i = 0; i < 10000000; i++) { arg->p[0]++; }

  printf("g = %ld, s = %ld, l = %ld, c[0] = %ld, p[0] = %ld\n",
	 g, s, l, arg->c[0], arg->p[0]);

  return 0;
}
</pre>

この関数(thread_func)を実行するスレッドを以下のように多数作った時,
<pre>
int main()
{
  struct thread_arg args[N_THREADS];
  long a_counter[1] = { 0 };
  int i;

  /* スレッドを N_THREADS 個作る */
  for (i = 0; i < N_THREADS; i++) {
    pthread_create(&args[i].tid, NULL, 
		   thread_func, (void *)&args[i]);
    args[i].idx = i;
    args[i].c[0] = 0;
    args[i].p = a_counter;
  }
  /* 終了待ち */
  for (i = 0; i < N_THREADS; i++) {
    pthread_join(args[i].tid, NULL);
  }

}
</pre>

<p>
<font color=red>問題:</font>
競合状態(race condition)を作っているのはどの更新か
(s++? g++? ...)? それぞれのプログラムを実行すると，
printf("g = ...") で表示される値はどのようになるか?
予想した上で実行せよ．
</p>
<p>また，各スレッドでそれぞれの更新されている場所
のアドレスを表示してみよ．上記の結果と一貫していることを確かめよ．
</p>

</div>

<h2>競合状態解消の演習</h2>
<div>
<p>
<font color=red>問題:</font> 先のプログラムに排他制御の呼び出し
(マニュアルで, pthread_mutex_lock と関連する関数を調べよ)を加えて,
このプログラムを修正し，全ての変数が正しく更新される
(最終的に，行われた更新の回数だけ，変数が増加する)ようにせよ．
</p>

<p>
<font color=red>問題:</font> 
<a href=spin.h>このファイル</a>をincludeすると，
以下のデータ構造と, 3つの関数が使えるようになる．
<ul>
  <li><t>spinlock_t s;</t></li>
  <li><t>int spin_init(spinlock_t * s, int pshared);</li>
  <li><t>int spin_lock(spinlock_t * s);</li>
  <li><t>int spin_trylock(spinlock_t * s);</li>
  <li><t>int spin_unlock(spinlock_t * s);</li>
</ul>

それぞれ pthread_mutex_t, pthread_mutex_init, 
pthread_mutex_lock, pthread_mutex_trylock, 
pthread_mutex_unlockと同じ意味を持つが，
違いは，spin_lockはlockが取得されるまで
頻忙待機で待つことである．ちなみにspin_lock, spin_unlock
の中身は以下のような感じ．
<pre>
int spin_lock(spinlock_t *lock) {
  volatile int * x = &lock->locked;
  while (1) {
    while (*x) { }
    if (__sync_bool_compare_and_swap(x, 0, 1)) {
      __sync_synchronize();
      return 0;
    }
  }
}

int spin_unlock(spinlock_t *lock) {
  assert(lock->locked);
  __sync_synchronize();
  lock->locked = 0;
  return 0;
}
</pre>

前問題の，mutexを用いる代わりに，
spinlockを用いて同じ問題を解決せよ．
</p>

<p>
<font color=red>問題:</font> 多くのプロセッサには,
メモリへの可算を不可分に行う命令や,
授業で説明した, compare-and-swap 命令が備わっている.
これらをつかうと排他制御をしなくても(一つのアドレスに対する)
不可分な更新を行うことができる.
</p>

<p>
gccではそれぞれ,
<ul>
<li> <font color=blue>__sync_fetch_and_add(T * p, T val);  </font>
<br /> *p += val を不可分に
</li>
<li> <font color=blue>__sync_val_compare_and_swap(T * p, T old, T new); </font>
<br /> if (*p == old) { *p = new; return old; } else { return new; } を不可分に
</li>

<li> <font color=blue>__sync_bool_compare_and_swap(T * p, T old, T new);  </font>
<br /> if (*p == oldval) { *p = new; return 1; } else { return 0; } を不可分に
</li>
</ul>
という関数として利用できる. 詳しくはgccのinfo (コマンドでinfo gccまたはEmacsで M-x info. 表示されなければ sudo apt-get install gcc-docでインストール)などを参照する. 

mutexやspinlockを使わず，
__sync_bool_compare_and_swapまたは
__sync_val_compare_and_swapを利用して
同じ問題を解決してみよ．
</p>

<p>注:
この問題ではあきらかに__sync_fetch_and_addを使うのが簡単だが,
compare_and_swapは色々な場面で有用なプリミティブなので,
練習すると良い. 
また, mutex_lockやspin_lockを利用したものと時間(速度)を比較してみよ.
</p>

<p>
<font color=red>問題:</font> 
都合，同じ問題を解決するのに，
<ul>
  <li>mutexを用いたバージョン，</li>
  <li>spinlockを用いたバージョン，</li>
  <li>compare_and_swapを用いたバージョン，</li>
</ul>
の3通りができた．スレッド数を色々変えて，性能を比較してみよ．特に，
<ul>
  <li>spinがmutexよりも大きく負けるのはどのようなスレッド数の時か</li>
  <li>spinがmutexに勝るのはどのようなスレッド数の時か</li>
  <li>それらのスレッド数で，
    compare_and_swapを用いたバージョンの性能はどうか</li>
</ul>
予想してから，確かめてみよ．
また，
<pre>
taskset -c 0 ./pthread_inc_mutex 4 1000000
</pre>
などとすると，4スレッドを作りつつも，それらを一つのCPU (この場合はCPU 0番)
に割り当てることができる．
この設定のある，なしで性能がどう変わるかも予想・実験せよ．
<ul>
</ul>
</p>
</div>



<!--
<h2>スレッド間での値の受け渡し (生産者消費者同期)</h2>

<h3>概要</h3>
スレッドAからスレッドBに値を送るには, 
メモリ上のどこかで共有する場所を決め,
Aがその場所に書き, Bがその場所から読み出すのが基本である.
しかし, 正しく同期をとって, 
前者が後者よりも先に実行されるようにしないと,
値は正しく行き渡らない.

ここでは簡単な例題でこれを確認し, 正しい同期の方法を会得しよう. 
例題: 以下を繰り返すプログラムを作る.

<ul>
<li>2つのスレッドA, Bを作り, 以下を繰り返す.
<ol>
<li>スレッドAが変数 x に適当な値を書き込む
<li>スレッドBがそれを変数 x から読み出す
<li>スレッドBはそれを読み出したら読み出された値の2乗を変数 y に書き込む
<li>スレッドAがそれを変数 y から読み出す
<li>これを多数回繰り返す
</ol>
</ul>

値が正しく読み込まれていることを確認するために,
ループの第 i 回目に x に書き込まれる値が, i など,
簡単な規則を決めておけば良い.

まずは一切同期をせずに走らせると何が起きるかを見てみよ.
<a href=pthread_producer_consumer.c>
pthread_producer_consumer.c</a> (Pthreadを用いたもの)
または
<a href=openmp_producer_consumer.c>
openmp_producer_consumer.c</a> (OpenMPを用いたもの)
をダウンロードして, コンパイル, 実行せよ.

<h3>頻忙待機を用いた同期</h3>

「概念的に簡単」「一応同期は正しく取れる」

-->

<h2>排他制御と条件変数を用いた「正しい」データ構造の更新</h2>
<div>
<p>
<a href=pthread_queue.c>pthread_queue.c</a> (Pthreadsを用いたもの)
または<a href=openmp_queue.c>openmp_queue.c</a> (OpenMPを用いたもの)
は, FIFOキューを作り,
いくつかののスレッドが多数(例えば1000万個)の要素(非負の整数)を挿入(enq)し, 
それと並行していくつかのスレッドが同じFIFOキューから挿入された要素を取り出す
(deq)というプログラムである. 具体的にはコンパイルして,
<pre>
./a.out N E D                          # pthreadの場合
OMP_NUM_THREADS=EとDの和 ./a.out N E   # openmpの場合 (E+Dは自分で計算する. Dは, OMP_NUM_THREADSとEから計算される)
</pre>
として走らせると,
<ul>
<li> FIFOキューを一つ作る
<li> E 個の挿入(enq)役スレッドを作る
<li> D 個の削除(deq)役スレッドを作る
<li> E 個の挿入スレッドはそれぞれ約(N / E)個の要素を FIFO キューに挿入
  (端数処理を適切に行い, 合計でぴったりN個挿入する)
<li> D 個の挿入スレッドはそれぞれ約(N / D)個の要素を FIFO キューから削除
  (端数処理を適切に行い, 合計でぴったりN個削除する)
</ul>
という動作をする. 挿入した要素が一度ずつ削除されれば成功で,
プログラムはそれをチェックする.
</li>

<p>
しかし,
<ul>
<li>データ構造更新のための排他制御も行なっていない
<li>deqは, キューが空の場合, -1を返すようになっている
</ul>
という理由でそのまま走らせてもうまく動かない (assertionエラーで終了する).
</li>

<p>
これを正しく動くように変更せよ.
</p>

<h3>ステップ1: 確認</h3>
<p>
まず, 全部のスレッドを逐次的に動かしてみたら正しく動くことを確認せよ.
それには, pthread_queue.c でスレッドを作った直後に終了待ち(join)を
するようにすれば良い. OpenMPであれば, #pragma omp parallel を,
単なる逐次的なループに置き換えれば良い.
</p>

<h3>ステップ2: 排他制御 + 頻忙待機</h3>
<p>
排他制御でデータ構造が正しく変更されるようにせよ.
deq時にキューが空だった場合に-1を返す部分は, 
空で無くなるまで空回りする(頻忙待機).
</p>

<h3>ステップ3: 条件変数を用いた同期</h3>
<p>
deq時にキューが空だった場合は,
空で無くなるまで, 
条件変数を用いてブロック(中断), 休眠待機するようにする.
</p>
</div>

<div>
<font color=red>問題:</font> 
<p>
ステップ2, およびステップ3の方法を実装し, 
ともにただしく動くことを確認せよ.
また，ステップ2で行った方式の問題点を指摘せよ.
(ただしこの実験の設定ではステップ2で行った方法でも,
性能上の問題は起きない. 余力があれば考えてみよ)
</p>

<!--
<p>
<iframe src="https://docs.google.com/forms/d/1ZPX4YlrGA_vt9Cc1gR-SS6Bj98DfUb0eTBvWJ55-_1g/viewform?embedded=true" width="760" height="800" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
</p>
-->

</div>


<h2>簡単な並列処理</h2>
<div>
<p>
sin(x)を0からPI/2まで積分をする逐次プログラムがある. 
(<a href=int_seq.c>逐次プログラム</a>)
</p>

<p>
<font color=red>問題:</font> pthreadまたはOpenMPを用いて並列化してみよ
(OpenMPを使う場合以下
<a href=#openmp_for>ループを並列化するためのOpenMPの構文</a>を参照).
<ul>
  <li>integral関数のiのループを, 2スレッド, 3スレッド, 4スレッド, ...で等分し, 1スレッドの時と実行時間を比べよ.</li>
  <li>この例題は, 1次元を1億等分するという, 計算方法としてはおろかなもの. 
    普通はもう少し誤差の少ない積分方法を用いて, 無闇に細かい区間に分けないで良いように計算する.
    実際には, 積分計算に時間がかかるようになるのは多次元関数の場合
    (どうしても区間の数が多くなる).</li>
</ul>
</p>

<!--
<p>
<iframe src="https://docs.google.com/forms/d/1UAazNhDQPu5JzwKXfKydkMDjSnrq2y_Ck8k9QN7rliI/viewform?embedded=true" width="760" height="800" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
</p>
-->

<a name=openmp_for>
<h3>ループを並列化するためのOpenMPの構文</h3>
<p>
OpenMPには, for文を並列化するための簡便な構文がある.
</p>

<p>
<blockquote>
<pre>
#pragma omp parallel
#pragma omp for
  for (i = 0; i < n; i++) {
     ...
  }
</pre>
</blockquote>
のように書くことで, #pragma omp for の直下に書かれたfor文を
並列に実行してくれる. ただし, それらを並列化しても実行が「正しく」
行われることを保証するのはプログラマの責任である.
</p>

<p>
上記のプログラムではだいたい以下のようなことが起こると思えば良い.
<ul>
<li>#pragma omp parallelに遭遇したのでスレッドがいくつか(OMP_NUM_THREADS
で指定した分だけ)作られる</li>
<li>それぞれのスレッドが#pragma omp for に遭遇する. ここでは, その下の
for文のiteration (...部)を, スレッド間で分割する(work sharing)</li>
<li>既定の動作は全iterationをスレッド間で頭から等分するという物.
Pをスレッド数として, 最初のN/Pをスレッド0, 次のN/Pをスレッド1, という具合
</li>
</ul>
</p>

</div>

<h2>動的負荷分散を必要とする例題</h2>
<div>
<p>
2以上からN未満の素数の個数を数える逐次プログラムがある(
<a href=prime_seq.c>逐次プログラム</a>).
</p>

<font color=red>問題:</font>
<p>
このプログラムをpthreadまたはOpenMPを用いて並列化してみよ.
以下の, 動的負荷分散の方法を参考にせよ.
</p>

<p>
<ul>
<li><font color=brown>
    全数に対して素数か否かを検査するnのループを等分して1スレッドの時と実行時間を比べよ.</font></li>
<li>先の例と異なり, 各nに対する負荷は一定ではない. 
nのループを等分しただけでは各スレッドの仕事量は等しくならない.
  実際にはこのようなケースがしばしば生ずる.</li>
</ul>
</p>

<!--
<p>
<iframe src="https://docs.google.com/forms/d/1BvQSeQXqB2IwX42Ga9jNtGptjAzz_NCXnF1c3kPgoOE/viewform?embedded=true" width="760" height="800" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
</p>
-->

<h3>動的負荷分散の方法</h3>
<p>
一般に, スレッド数は少なすぎると負荷の偏りを吸収できない可能性があり,
かと言って多すぎるとオーバーヘッドが大きくてやはり思うように速度向上しない.
プログラミングは面倒になるが以下のような方法が汎用的な方法として使われる.
<ul>
<li>CPU数と同じ数のスレッド数を作る.</li>
<li>ある程度小さな処理の単位を, 多数生成する.</li>
<li>スレッドがタスクを次々と処理していく.</li>
</ul>
</p>

<p>
素数の例で言えば, 
<ul>
<li>適当な大きさの区間をタスクとする.
<pre>
typedef struct prime_task
{
  int a;
  int b;
   ...
} * prime_task_t;
</pre>
このようなprime_taskの配列を作る.</li>
<li>スレッドは小数(普通はCPU数と同じにする)作る. 各スレッドはそのprime_taskの配列を受け取る.</li>
<li>各スレッドはprime_taskの配列から, 「まだ他のスレッドが手をつけていないタスク」を取って,
  その部分の結果を出す.</li>
<li>「まだ他のスレッドが手をつけていないタスク」は適当な要素を上記の構造体に持たせればよいが,
二つ以上のスレッドが同じタスクを実行してしまわないよう,
  排他制御を行う必要がある.</li>
</ul>
</p>

<h3>OpenMPにおけるfor文の動的負荷分散</h3>
<p>
実はこの例であれば, OpenMPのparallel for文を使うと簡単に処理できる.
<blockquote>
<pre>
#pragma omp parallel
#pragma omp for <font color=red>schedule(dynamic)</font>
  for (i = 0; i < n; i++) {
     ...
  }
</pre>
</blockquote>
とすると, for文のiterationをスレッドに分割する方法が変更され,
動的な分割(たいてい, 「暇なスレッドが次のいくつかのiterationを取っていく」という方式)
に変わる.
</p>
</div>

<h2>応用問題</h2>
<div>
<p>
多少高度な例題として, 多項式f(z)の根を, 複素数を含めてすべて見つけるアルゴリズムを考える.
原理は, 複素平面上の領域を考え, その中にf(z)=0なる点が存在する範囲を徐々に狭めていくという原理である.
それをやるには, 「この領域にはf(z)=0となる点は存在しない」ということが判定できなくてはならない.
もう忘れたとは思うが, それを判定するために, 複素解析で有用な定理がある. それは,
偏角の原理(<a href=arg.pdf>ここ</a>をクリック)
というもの. つまり, ある領域の境界にそって f'(z)/f(z) を積分し, それが0ならその領域にはf(z)=0
となる点はないという事が分かる.
<ul>
  <li><a href=find_roots_seq.c>逐次プログラム</a></li>
</ul>
</p>

<p>
この問題では, タスクを実行時に細分化していく必要がある. 最終的に求めたいのは(ある誤差 e 以内で),
根が含まれる矩形領域である. したがって一辺e以内の矩形領域を最初から多数生成すれば,
積分の例と同じように解くことができる.
</p>
<p>
しかしそれでは矩形領域の数が多くなりすぎる.
さらに, より大きな矩形で積分して, それが0であればその中には根がないと
わかるのだから, 最初から小さな矩形領域を使うのは無駄というものである.
</p>
<p>
そこで最初は大きな矩形領域で積分し, その積分が0でないときのみ, 
その領域を細分化するという方法が考えられる.
その方法を素直に再帰的な関数として書いたものが, 
上記の逐次プログラムであるが, pthreadを用いてこれを並列化するには
かなりの書き換えが必要になる.
</p>

<p>
<font color=red>問題:</font> このプログラムを並列化してみよ.
このプログラムの並列化が出来たら初心者は卒業と言える．
</p>

<!--
<p>
<iframe src="https://docs.google.com/forms/d/1pSamTGKNnxNmZ6pnwRVQWdJ8mBmocwYgAi1X4coHOYU/viewform?embedded=true" width="760" height="800" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>
</p>
-->

</div>

<!--

<h2>課題</h2>
<ul>
<li><font color=red>締切り: 12/7(月) 23:59</font>
<li>内容: 以下で赤字(茶色字)で指示のある部分についてプログラムを作成. 
ただし「高度な課題」となっている所は推奨だがオプションとする.
<li>レポートの内容: 詳細は近々(数日以内)にここに書きますが, 
プログラムの動作確認, 性能(並列化による速度向上)測定を行う.
<li>提出方法: 下記「レポートの形式と内容」にしたがったレポート,
プログラム一式を一つのディレクトリに納め, 
tar.gz形式のファイルにまとめて添付し,
最低限以下のような情報を含んだ形でメールする. 
もちろん「g1234567」の部分は自分の学生証番号に
置き換え, 「三村マサカズ」の部分は自分の名前に置き換える.
<blockquote>
<pre>
To: os-report@logos.ic.i.u-tokyo.ac.jp
Subject: g1234567 os report

g1234567 三村マサカズ
です.

OSのレポートを提出します.
</pre>
</blockquote>

tar.gz形式のファイルの作り方は以下のとおり.
<blockquote>
<pre>
$ たとえばディレクトリos-reportにすべてのファイルを収める.
$ tar cvf os-report.tar os-report
os-report/
os-report/int_para.c
os-report/find_roots_para.c
os-report/report.pdf
os-report/prime_para.c
$ gzip os-report.tar 
</pre>
</blockquote>
これで, os-report.tar.gzというファイルが出きるはずである.
</ul>

<h2>レポートの形式と内容</h2>
<ul>
<li>ファイル形式は.pdf形式にしてください.
<li>実行した課題の簡潔なリスト
<li>それぞれの説明. 並列化の方法とその台数効果(といっても, 
要するに2台でどれだけ速くなったかということ). 
なるべく2倍近く速くなるよう努力してください.
思うように台数効果が出なかった場合はその理由の考察.
もし, 最初に試した方法で台数効果が出ず, 
それを改良したという場合, その過程も書いてください(それ自身が今回学んだ事でしょうから).
</ul>
-->


<!-- コンテンツはここまで -->

</body>
</html>
