<!--- md ---> 

#* オペレーティングシステム 演習 08

#* Compare-and-swap / 各種不可分更新方法のの可視化

<!--- end md --->

<!--- md w ---> 

名前と学生証番号を書け. Enter your name and student ID.

 * 名前 Name:
 * 学生証番号 Student ID:

<!--- end md --->


<!--- md ---> 
# 始めに

* 1変数に対する不可分な更新を行うのには排他制御を使う方法と不可分更新命令を使う方法がある
* 排他制御にも休眠待機を行うもの(pthread_mutex_t)と, ビジーウェイトを行うもの(pthread_spinlock_t)がある
* 変数の更新を行う3つの方法を試してその挙動を可視化しよう

* 以下はどれも全スレッドで合計n回, 

```
g = compute(g, l);
```

という文を(gを読み出してから更新するまでを)不可分に実行する. 排他制御を使う場合は,
```
lock();
g = compute(g, l);
unlock();
```

compute(g) の中身に特に意味はないが,
```
double compute(x, l) {
  for (i = 0; i < l; i++) {
    x = a * x + b
  }
  return x;
}
```

* 可視化をしやすくするため, クリティカルセクションの外で適当な時間だけsleepできるようにする
```
./lock_rec_xxx nthreads n l s
```
で, 
```
/* Nスレッド合計でn回 */
for (long i = 0; i < n; i++) {
    g = compute(g, l); // 不可分に
    s秒休眠
}
```
という動きをする
<!--- end md ---> 

<!--- md ---> 
# 可視化のためのプログラム
<!--- end md ---> 

<!--- code w kernel=python --->
<!--- include nb/source/include/os08_cas/lock_rec_vis.py --->
<!--- end code --->

<!--- md ---> 
# mutex

* まずは最も基本的な, mutex を使う方法
<!--- end md ---> 

<!--- code w kernel=python --->
%%writefile lock_rec_mutex.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/include/os08_cas/lock_rec.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
gcc -Wall -fopenmp -o lock_rec_mutex lock_rec_mutex.c
<!--- end code --->

<!--- md ---> 
* 実行
<!--- end md ---> 

<!--- code w kernel=bash --->
nthreads=4
n=100000
l=1000
s=1.0e-6
set=2-5
/usr/bin/time taskset -c ${set} ./lock_rec_mutex ${nthreads} ${n} ${l} ${s} > mutex.dat
<!--- end code --->

<!--- md ---> 
* 可視化
* start_t= と end_t= を指定して, 特定の区間だけを表示する(適宜変更してみよ)
<!--- end md --->

<!--- code w kernel=python --->
start_t = 0
end_t = float("inf")
lock_wait_gantts(["mutex.dat"], start_t=start_t, end_t=end_t)
progress_threads_plts(["mutex.dat"], start_t=start_t, end_t=end_t)
compare_progress_plts(["mutex.dat"], start_t=start_t, end_t=end_t)
<!--- end code --->

<!--- md --->
* <font color="purple">上記でパラメータを色々変えて試してみよ</font>

* taskset -c 2-5 は CPU 2-5 (本当は仮想コア) を使うという意味.
* N CPU (本当は仮想コア)がある環境では, 0 〜 $N - 1$ までの仮想コア番号を指定できる
* /usr/bin/time というコマンドでプロセスの使用したCPU時間を表示している

```
0.67user 0.20system 0:00.67elapsed 129%CPU (0avgtext+0avgdata 5140maxresident)k
0inputs+14240outputs (0major+962minor)pagefaults 0swaps
```

は ユーザ時間が0.67秒, システム(OS)時間が0.20秒消費したことを示している

* 注目点
  * 1つめのグラフは各スレッドがクリティカルセクションの中を実行している部分を濃い太線で表示している. クリティカルセクションが時間的に重なっていないことを確認せよ
  * mutex lockに入ってからリターンするまで(つまり, lockを待っている時間)を薄い線で表示している. スレッドが増えるとほとんどの時間が待ち時間になることを観察せよ
  * 使用CPU数を固定してスレッド数nthreadsを増やすと結果(経過時間, CPUの消費時間)がどう変わるかを試してみよ
  * 同じスレッド数でCPU数を増やすとどうなるか?
<!--- end md --->

<!--- md --->
# spinlock

* spinlock は排他制御と同じだが, lockが獲得できるまでブロックせずにビジーウェイトする
<!--- end md --->

<!--- code w kernel=python --->
%%writefile lock_rec_spin.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/include/os08_cas/lock_rec.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
gcc -Wall -fopenmp -o lock_rec_spin lock_rec_spin.c
<!--- end code --->

<!--- code w kernel=bash --->
nthreads=4
n=100000
l=1000
s=1.0e-6
set=2-5
/usr/bin/time taskset -c ${set} ./lock_rec_spin ${nthreads} ${n} ${l} ${s} > spin.dat
<!--- end code --->

<!--- code w kernel=python --->
start_t = 0
end_t = float("inf")
lock_wait_gantts(["spin.dat"], start_t=start_t, end_t=end_t)
progress_threads_plts(["spin.dat"], start_t=start_t, end_t=end_t)
compare_progress_plts(["spin.dat"], start_t=start_t, end_t=end_t)
<!--- end code --->

<!--- md --->
* <font color="purple">同様にパラメータを変えて試してみよ</font>

* 注目点
  * CPUの利用時間 (/usr/bin/timeが表示する)
  * スレッド数 &gt; コア数(tasksetで指定したCPU数)の場合の挙動
  * スレッド数 &lt;= コア数の場合の経過時間 (mutexの場合との比較)
  * spinlockの方がmutexに勝る, 負けるのはどう言う場合か?
<!--- end md --->

<!--- md --->
# 不可分更新 (compare-and-swap)

```
g = compute(g, l)
```
を不可分に行うために, lockを用いずにcompare-and-swap命令を用いて以下のようにする.
排他制御を行っていないので, lockの待ち時間は存在しない.

```
while (1) {
  double x = g;
  double y = compute(x, l);
  if (compare_and_swap(&g, x, y)) break;
}
```
<!--- end md --->

<!--- code w kernel=python --->
%%writefile lock_rec_cas.c
<!--- exec-include ./mk_version.py -D VER=3 nb/source/include/os08_cas/lock_rec.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
gcc -Wall -fopenmp -o lock_rec_cas lock_rec_cas.c
<!--- end code --->

<!--- code w kernel=bash --->
nthreads=4
n=100000
l=1000
s=1.0e-6
set=2-5
/usr/bin/time taskset -c ${set} ./lock_rec_cas ${nthreads} ${n} ${l} ${s} > cas.dat
<!--- end code --->

<!--- code w kernel=python --->
start_t = 0
end_t = float("inf")
lock_wait_gantts(["cas.dat"], start_t=start_t, end_t=end_t)
progress_threads_plts(["cas.dat"], start_t=start_t, end_t=end_t)
compare_progress_plts(["cas.dat"], start_t=start_t, end_t=end_t)
<!--- end code --->

<!--- md --->
* <font color="purple">同様に上記のパラメータを変えて試してみよ</font>

* 注目点
  * CPUの利用時間 (/usr/bin/timeが表示する)
  * スレッド数 &lt;= 使用CPU数のときの挙動
  * スレッド数 &gt; 使用CPU数のときの挙動
<!--- end md --->

<!--- md label=prob,ans --->
#*P 3者の比較
<!--- end md --->

<!--- md label=prob,ans --->
* 3通りの方法を同じパラメータで実行してそれらを可視化して比較せよ
* パラメータを色々変化させてどのように結果が変わるかを調べよ
<!--- end md --->

<!--- code w kernel=bash label=prob,ans points=1 --->
nthreads=4
n=100000
l=1000
s=1.0e-6
set=2-5
/usr/bin/time taskset -c ${set} ./lock_rec_mutex ${nthreads} ${n} ${l} ${s} > mutex.dat
/usr/bin/time taskset -c ${set} ./lock_rec_spin ${nthreads}  ${n} ${l} ${s} > spin.dat
/usr/bin/time taskset -c ${set} ./lock_rec_cas ${nthreads}   ${n} ${l} ${s} > cas.dat
<!--- end code --->

<!--- code w kernel=python label=prob,ans points=1 --->
start_t = 0
end_t = float("inf")
compare_progress_plts(["mutex.dat", "spin.dat", "cas.dat"], start_t=start_t, end_t=end_t)
<!--- end code --->

<!--- md label=prob,ans --->
* spinlock の方が mutex よりも有利なのはどういう状況か?
* spinlock が不利になるのはどういう状況か? 可視化された結果を元に何が起きているのかを述べよ
* その状況で compare-and-swap を使う方法では何が起きるか? 不利にならないのはなぜか?
<!--- end md --->

<!--- md w points=1 --->
(解答欄)



<!--- end md --->

<!--- md label=ans --->

* spinlock の方が mutex よりも有利なのは, 各スレッドがずっとCPUを割り当てられるような状態(更新をしているスレッド数 $\leq$ CPU数で, 他にCPUを消費しているスレッドがほとんどいないような状態)

* spinlock が不利になるのはそうでない状態. 特に, スレッド数 $>$ CPU数であるような状態 (「CPU数」はtasksetで, スレッドが実行されるCPUが一部に限定されている場合はそのCPU数). ロックを保持しているスレッド(Aとする)がpreemptionを受けて他のスレッドを実行すると, 再びAが実行される(次のpreemptionが起きる)まで, どのスレッドも更新ができなくなる. これは可視化結果で, カウンタの増加が止まる時間帯が断続的に生ずることから確認できる

* compare-and-swap を使う方法でも, あるスレッド(Aとする)が, gを読み出してから変更後の値を書き込むまでの間にpreemptionを受けることがあり, その場合Aはその変更をやり直すことになるが, その間も他のスレッドは値を更新することができる
<!--- end md --->

